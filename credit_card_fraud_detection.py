# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sBPL0Iffvj0iaZ4-P9UrbXSNn_D8GuNW
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

#Data to pandas Dataframe
credit_card_data=pd.read_csv('/content/credit_data.csv')

credit_card_data.head()

credit_card_data.tail()

credit_card_data.info()

#checking the number of null or missing values in each columns
credit_card_data.isnull().sum()

#in the column named "class" has two values 0 and 1
# 0 means valid/Legit/normal transaction
# 1 means frdulent transaction

#Here we find the number of transaction which are legit and fraud

credit_card_data['Class'].value_counts()

#There are 284315 legit and 492 fraud transaction

# separate The Dataset into valid and fraud subDataset

legit=credit_card_data[credit_card_data.Class==0]
fraud=credit_card_data[credit_card_data.Class==1]

print(legit.shape)
print(fraud.shape)

legit.Amount.describe()

fraud.Amount.describe()

"""Now we can see that there is Noticible Diffreence in the mean value in legit and fraud subDataSet"""

# compare the all column values of the main data set wrt to the 0 and 1 entry of the Class colum

credit_card_data.groupby('Class').mean()

"""# **bold text**
For the Accurate data training and prediction we need the the similar distribution of both data for the legit and Fraud

so for this , we come to know that  no. Fraud is 492 so we take 492 legit transaction and combine them.
"""

legit_sample=legit.sample(n=492)
# Fraud sample / data is already made in the fraud varible

new_dataset=pd.concat([legit_sample,fraud], axis=0)  # [dataset1,dataset2]

new_dataset.head()

new_dataset.tail()

new_dataset['Class'].value_counts()

#Spiting the data into the Input and Output
# X -> MODEL -> Y
# X input parameter and Y the output Label
#Given (X) → Predict (Y)
X=new_dataset.drop(columns='Class',axis=1) #axis=1 means removing the colum( vertically)
Y=new_dataset['Class']

print(X)
# X.head()

print(Y)

X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=2)

"""test_size=0.2
This means 20% of the data will go into the test set, and the remaining 80% will be used for training
Example:
If you have 100 data points:

80 will go into training (X_train, Y_train)

20 will go into testing (X_test, Y_test)

----------------------------
random_state=2
This sets a seed for the random number generator.

It ensures that the split is reproducible — you’ll get the same train-test split every time you run the code.

Different values (e.g., random_state=42, random_state=1) will give different splits, but using a fixed number ensures consistent results.

-------
Ye classification problems mein bahut important hota hai, jab classes imbalanced hoti hain.

Jaise agar aapke labels mein:

60% class 0

40% class 1 hai

To stratify=Y ensure karega ki:

Training aur testing dono mein bhi approx 60% class 0 aur 40% class 1 hi ho.

Agar stratify=Y nahi doge, to ho sakta hai test ya train set mein sirf ek hi class zyada ho jaye — jiski wajah se model galat training le sakta hai.


"""

print(X.shape,X_train.shape, X_test.shape)

"""MODEL TAINING

LOGISTIC REGRESSION
"""

model=LogisticRegression()

X_train_prediction=model.predict(X_train)
training_data_accuracy=accuracy_score(X_train_prediction,Y_train)

print('Accuracy on Training data : ',training_data_accuracy)